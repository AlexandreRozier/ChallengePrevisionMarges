{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import warnings\n",
    "from cgi import test\n",
    "from pathlib import Path\n",
    "import time\n",
    "from unittest import result\n",
    "from pytorch_lightning import Trainer\n",
    "from ray import tune\n",
    "import torch\n",
    "\n",
    "from train import DataModule, Regressor\n",
    "\n",
    "sn.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "ALLOWED_HORIZONS = [0.5,1,2,4]\n",
    "\n",
    "other_cols = [\"tcc\",\"t2m\",\"ssrd\",\"ff100\",\"u100\",\"v100\"]\n",
    "data_folder = \"./data_challenge/data\"\n",
    "def percent_rows_na(df):\n",
    "    return (len(df)-len(df.dropna(axis=0)))*100/len(df)\n",
    "\n",
    "\n",
    "def assign_meteo_date_lancement(dt):    \n",
    "    \n",
    "    if 0<= dt.hour < 6:\n",
    "         hour_date_lancement = 0\n",
    "    elif 6< dt.hour <= 12:\n",
    "         hour_date_lancement = 6\n",
    "    elif 12< dt.hour <= 18:\n",
    "         hour_date_lancement = 12\n",
    "    else:\n",
    "         hour_date_lancement = 18\n",
    "    return dt.replace(hour=hour_date_lancement)\n",
    "         \n",
    "def fix_echeance(df):\n",
    "    df['echeance'] = (df.date_cible - df.date_lancement).dt.seconds/3600\n",
    "\n",
    "def remove_useless_horizons(df):\n",
    "    return df.loc[df.echeance.isin(ALLOWED_HORIZONS)]\n",
    "\n",
    "def add_datetime_features(df):\n",
    "    # time in the year\n",
    "    #df['year_dt'] =  datetime.datetime(year=df.date_cible.dt.year)\n",
    "    tzinfo = df.date_cible.dt.tz\n",
    "    df['tiy'] = (df.date_cible - df.date_cible.dt.year.apply(lambda y: datetime.datetime(year=y,month=1,day=1,tzinfo=tzinfo))).dt.total_seconds()/(365*24*60*60)\n",
    "    # time in the day\n",
    "    df['tid'] = (df.date_cible.dt.hour *3600 + df.date_cible.dt.minute *60 + df.date_cible.dt.second)/(24*60*60)\n",
    "    # TODO: type of day for consumption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_station = pd.read_csv(os.path.join(data_folder,\"liste_stations.csv\"), sep=\";\", header=0)\n",
    "df_list_station.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prev_sans_obs2020 = pd.read_feather(os.path.join(data_folder, \"df_prev_sans_obs2020.feather\"))\n",
    "print(df_prev_sans_obs2020.echeance.unique()) # echeance 30min - 7h\n",
    "print(df_prev_sans_obs2020.isnull().sum()) # Missing 417844 observations (for 2020)\n",
    "# Add fake PI for conso in order to get a FC between 0 and 1\n",
    "df_prev_sans_obs2020.loc[df_prev_sans_obs2020.type.str.contains('conso'),'pi'] = df_prev_sans_obs2020[df_prev_sans_obs2020.type.str.contains('conso')].obs.max() + 10**5\n",
    "# Compute FCs\n",
    "df_prev_sans_obs2020['fc'] = df_prev_sans_obs2020['obs'] / df_prev_sans_obs2020['pi']\n",
    "\n",
    "add_datetime_features(df_prev_sans_obs2020)\n",
    "df_prev_sans_obs2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grille_zoneclimat_fin18 = pd.read_feather(os.path.join(data_folder, \"grille_zone_climatique_fin2018.feather\"))\n",
    "df_grille_zoneclimat_fin18.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo_zone_eol = pd.read_feather(os.path.join(data_folder, \"meteo_zone_echeance12_2016_2020_HRES_piEOL_smooth.feather\"))\n",
    "df_meteo_zone_eol.groupby('date_lancement_meteo').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo_zone_eol = pd.read_feather(os.path.join(data_folder, \"meteo_zone_echeance12_2016_2020_HRES_piEOL_smooth.feather\"))\n",
    "print(sorted(df_meteo_zone_eol.echeance.unique())) # echeance 0min - 11h30\n",
    "assert df_meteo_zone_eol.isnull().sum().sum() == 0 # No missing value\n",
    "# Long to large\n",
    "\n",
    "df_meteo_zone_eol = df_meteo_zone_eol.pivot(index=[\"date_lancement_meteo\",\"date_cible\",\"echeance\"], values=other_cols, columns=\"zone\").reset_index()\n",
    "assert df_meteo_zone_eol.isnull().sum().sum() == 0 # No missing value\n",
    "df_meteo_zone_eol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo_zone_pv = pd.read_feather(os.path.join(data_folder, \"meteo_zone_echeance12_2016_2020_HRES_piPV_smooth.feather\"))\n",
    "print(f\"echeances:{sorted(df_meteo_zone_pv.echeance.unique())}\") # echeance 0min - 11h30\n",
    "print(f\"zones:{sorted(df_meteo_zone_pv.zone.unique())}\") # echeance 0min - 11h30\n",
    "assert df_meteo_zone_pv.isnull().sum().sum() == 0 # No missing value\n",
    "\n",
    "\n",
    "# Long to large\n",
    "other_cols = [\"tcc\",\"t2m\",\"ssrd\",\"ff100\",\"u100\",\"v100\"]\n",
    "df_meteo_zone_pv = df_meteo_zone_pv.pivot(index=[\"date_lancement_meteo\",\"date_cible\",\"echeance\"], values=other_cols, columns=\"zone\").reset_index()\n",
    "assert df_meteo_zone_pv.isnull().sum().sum() == 0 # No missing value\n",
    "df_meteo_zone_pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prodpv_fc_q90 = pd.read_feather(os.path.join(data_folder, \"productionPV_FC_cielclair_q90.feather\"))\n",
    "df_prodpv_fc_q90.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#df = remove_useless_horizons(df_prev_sans_obs2020)\n",
    "df = df_prev_sans_obs2020\n",
    "\n",
    "# Drop uselss horizons\n",
    "df = df[df.echeance.isin([0.5,1,2,4])]\n",
    "\n",
    "df['date_lancement_meteo'] = df.date_lancement.apply(assign_meteo_date_lancement)\n",
    "df_conso = df[df.type =='consommation'].drop(columns='type')\n",
    "df_pv = df[df.type =='photovoltaique'].drop(columns='type')\n",
    "df_conso_res = df[df.type =='consommation_residuelle'].drop(columns='type')\n",
    "df_eol = df[df.type =='eolien'].drop(columns='type')\n",
    "# No missing data in year < 2020, prev\n",
    "assert percent_rows_na(df_eol[df_eol.date_cible.dt.year<2020])==0.0 # No missing value in train\n",
    "assert percent_rows_na(df_pv[df_pv.date_cible.dt.year<2020])==0.0 # No missing value in train\n",
    "assert percent_rows_na(df_conso_res[df_conso_res.date_cible.dt.year<2020])==0.0 # No missing value in train\n",
    "assert percent_rows_na(df_conso[df_conso.date_cible.dt.year<2020])==0.0 # No missing value in train\n",
    "\n",
    "# Meteo\n",
    "\n",
    "\n",
    "\n",
    "df_pv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PV\n",
    "PV_USELESS_COLS = ['ff100','u100','v100']\n",
    "df_pv_meteo = df_pv.merge(df_meteo_zone_pv.drop(columns=PV_USELESS_COLS+['echeance']), on=['date_cible','date_lancement_meteo'], how='inner').drop(columns='date_lancement_meteo')\n",
    "print(f\"\"\"\n",
    "      {percent_rows_na(df_pv_meteo)} % rows with missing values.\n",
    "      They come from merging meteo and prod/conso time series\n",
    "      \"\"\") \n",
    "df_pv_meteo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LONG\n",
    "EOL_USELESS_COLS = ['tcc','ssrd','t2m']\n",
    "df_eol_meteo = df_eol.merge(df_meteo_zone_eol.drop(columns=EOL_USELESS_COLS+['echeance']), on=['date_cible','date_lancement_meteo'], how='inner').drop(columns='date_lancement_meteo')\n",
    "# TODO check how many values are lost during inner join\n",
    "df_eol_meteo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSO_USELESS_COLS = ['ff100','u100','v100']\n",
    "# WARNING: TODO USE REAL WEATHER DATA \n",
    "\n",
    "\n",
    "df_conso_meteo = df_conso.merge(df_meteo_zone_pv.drop(columns=PV_USELESS_COLS+['echeance']), on=['date_cible','date_lancement_meteo'], how='inner').drop(columns='date_lancement_meteo')\n",
    "df_conso_meteo\n",
    "\n",
    "\n",
    "df_conso_res_meteo = df_conso_res.merge(df_meteo_zone_pv.drop(columns=PV_USELESS_COLS+['echeance']), on=['date_cible','date_lancement_meteo'], how='inner').drop(columns='date_lancement_meteo')\n",
    "df_conso_res_meteo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conso_meteo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv_meteo.to_hdf(\"./features/photovoltaique.hdf\",key=\"data\")\n",
    "df_eol_meteo.to_hdf(\"./features/eolien.hdf\",key=\"data\")\n",
    "df_conso_meteo.to_hdf(\"./features/consommation.hdf\",key=\"data\")\n",
    "df_conso_res_meteo.to_hdf(\"./features/consommation_residuelle.hdf\",key=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OBS_TYPES = ['photovoltaique','eolien','consommation','consommation_residuelle']\n",
    "\n",
    "def prepare_submission(obs_type, results):\n",
    "    \n",
    "    net=Regressor(results.best_config)\n",
    "    \n",
    "    # Predict quantiles - using dm.predict_loader would be cleaner but does not work....\n",
    "    with results.best_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "        ckp = torch.load(Path(loaded_checkpoint_dir) / \"checkpoint\")\n",
    "        net.load_state_dict(ckp['state_dict'])\n",
    "    \n",
    "    df = pd.read_hdf(f'./features/{obs_type}.hdf')\n",
    "    dm = DataModule(df, label='fc', batch_size=results.best_config['batch_size'])\n",
    "    dm.prepare_data()\n",
    "    net.eval()\n",
    "    outs = net(dm.x_test).detach()\n",
    "    \n",
    "    quantiles_cols = [f\"{level:.3f}\" for level in  np.array(Regressor(results.best_config).quantile_levels)]\n",
    "    quantiles_df = pd.DataFrame(columns=quantiles_cols, data=outs)\n",
    "    \n",
    "    # Concat to original DF\n",
    "    results_df = pd.concat([dm.df_test, quantiles_df.set_index(dm.df_test.index)],axis=1)\n",
    "    \n",
    "    # Remove useless echeances\n",
    "    results_df = results_df[results_df.echeance.isin([0.5,1,2,4])]\n",
    "    \n",
    "    # Large to long\n",
    "    results_df['id'] = results_df.index\n",
    "    results_df = results_df[['date_cible','date_lancement','pi']+quantiles_cols]\n",
    "    results_df =  pd.melt(results_df, id_vars=['date_cible','date_lancement','pi'],value_vars=quantiles_cols,var_name=\"quantile_niveau\",value_name=\"prev_q\")\n",
    "    results_df['quantile_niveau'] = pd.to_numeric(results_df['quantile_niveau'])\n",
    "    results_df['type'] = obs_type\n",
    "    \n",
    "    # Multiply by installed power \n",
    "    results_df['prev_q'] = results_df['prev_q'] * results_df['pi']\n",
    "    results_df.drop(columns='pi', inplace=True)\n",
    "    \n",
    "    # Zeroing negative productions\n",
    "    results_df.loc[results_df.prev_q < 0, 'prev_q'] = 0 \n",
    "    return results_df\n",
    "        \n",
    "\n",
    "outs = []\n",
    "for obs_type in OBS_TYPES:\n",
    "    exp_path = max(Path(f\"./ray/{obs_type}/\").glob('*experiment*'), key=os.path.getctime) # Hopefully take latest\n",
    "    results = tune.ExperimentAnalysis(experiment_checkpoint_path=exp_path,default_metric=\"val/loss\",default_mode=\"min\")\n",
    "    print(f\"\"\"\n",
    "          Preparing submission for {obs_type}...\n",
    "          Using Experiment {exp_path}\n",
    "          Validation loss: {results.best_result['val/loss']}\n",
    "          \"\"\")\n",
    "    print(results.best_config)\n",
    "\n",
    "    outs.append(prepare_submission(obs_type, results))\n",
    "submission = pd.concat(outs,axis=0)\n",
    "submission.reset_index(inplace=True)\n",
    "submission.drop(columns='index').to_feather(f'./submissions/AR_{time.time()}.feather')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.drop(columns='index').to_feather(f'./submissions/AR_{time.time()}.feather', compression=\"zstd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"plotly\"\n",
    "import plotly.express as px\n",
    "\n",
    "px.line(submission[submission.type=='consommation_residuelle'], x='date_cible',y='prev_q',color='quantile_niveau')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"plotly\"\n",
    "import plotly.express as px\n",
    "df_pv_plot = df_prev_sans_obs2020[(df_prev_sans_obs2020.date_cible.dt.year ==2019) & (df_prev_sans_obs2020.type==\"photovoltaique\")]\n",
    "df_pv_plot[df_pv_plot.echeance==0.0]\n",
    "px.line(df_pv_plot, x='date_cible',y='obs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDEAs\n",
    "- coeff zone meteo ? altitude ? longitude ?\n",
    "- is it damageable to scale features for quantile prediction ?\n",
    "- train only on useful horizons ?\n",
    "\n",
    "## TODO\n",
    "- add dvc \n",
    "- compute score for best models on val set \n",
    "- add prev to visualisation of outputs\n",
    "\n",
    "### features\n",
    "\n",
    "- ssrd\n",
    "- tcc\t\n",
    "t2m\t\n",
    "ssrd\t\n",
    "ff100\n",
    "u100\n",
    "v100\n",
    "echeance\n",
    "prod_installÃ©e \n",
    "puissance_installee\n",
    "zone_climatique\n",
    "zone\n",
    "clear_sky_FC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('prev_margins')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ca155f627546645520d33fc1c0c7b9e0b61bcb631c4abd47be2f89dc4da8d2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
