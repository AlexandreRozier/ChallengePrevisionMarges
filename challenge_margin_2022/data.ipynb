{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "%matplotlib inline\n",
    "sn.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "other_cols = [\"tcc\",\"t2m\",\"ssrd\",\"ff100\",\"u100\",\"v100\"]\n",
    "data_folder = \"./data_challenge/data\"\n",
    "def percent_rows_na(df):\n",
    "    return (len(df)-len(df.dropna(axis=0)))*100/len(df)\n",
    "\n",
    "def fix_echeance(df):\n",
    "    df['echeance'] = (df.date_cible - df.date_lancement).dt.seconds/3600\n",
    "    \n",
    "def add_datetime_features(df):\n",
    "    # time in the year\n",
    "    #df['year_dt'] =  datetime.datetime(year=df.date_cible.dt.year)\n",
    "    tzinfo = df.date_cible.dt.tz\n",
    "    df['tiy'] = (df.date_cible - df.date_cible.dt.year.apply(lambda y: datetime.datetime(year=y,month=1,day=1,tzinfo=tzinfo))).dt.total_seconds()/(365*24*60*60)\n",
    "    # time in the day\n",
    "    df['tid'] = (df.date_cible.dt.hour *3600 + df.date_cible.dt.minute *60 + df.date_cible.dt.second)/(24*60*60)\n",
    "    # TODO: type of day for consumption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_station = pd.read_csv(os.path.join(data_folder,\"liste_stations.csv\"), sep=\";\", header=0)\n",
    "df_list_station.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prev_sans_obs2020 = pd.read_feather(os.path.join(data_folder, \"df_prev_sans_obs2020.feather\"))\n",
    "print(df_prev_sans_obs2020.echeance.unique()) # echeance 30min - 7h\n",
    "print(df_prev_sans_obs2020.isnull().sum()) # Missing 417844 observations (for 2020)\n",
    "# Fake FC for conso\n",
    "df_prev_sans_obs2020.loc[df_prev_sans_obs2020.type.str.contains('conso'),'pi'] = df_prev_sans_obs2020[df_prev_sans_obs2020.type.str.contains('conso')].prev.max() + 10**4\n",
    "df_prev_sans_obs2020['fc'] = df_prev_sans_obs2020['obs'] / df_prev_sans_obs2020['pi']\n",
    "fix_echeance(df_prev_sans_obs2020)\n",
    "add_datetime_features(df_prev_sans_obs2020)\n",
    "df_prev_sans_obs2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grille_zoneclimat_fin18 = pd.read_feather(os.path.join(data_folder, \"grille_zone_climatique_fin2018.feather\"))\n",
    "df_grille_zoneclimat_fin18.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo_zone_eol = pd.read_feather(os.path.join(data_folder, \"meteo_zone_echeance12_2016_2020_HRES_piEOL_smooth.feather\"))\n",
    "print(sorted(df_meteo_zone_eol.echeance.unique())) # echeance 0min - 11h30\n",
    "assert df_meteo_zone_eol.isnull().sum().sum() == 0 # No missing value\n",
    "df_meteo_zone_eol.rename(columns={\"date_lancement_meteo\": \"date_lancement\"}, inplace=True)\n",
    "fix_echeance(df_meteo_zone_eol)\n",
    "# Long to large\n",
    "\n",
    "df_meteo_zone_eol = df_meteo_zone_eol.pivot(index=[\"date_lancement\",\"date_cible\",\"echeance\"], values=other_cols, columns=\"zone\").reset_index()\n",
    "assert df_meteo_zone_eol.isnull().sum().sum() == 0 # No missing value\n",
    "df_meteo_zone_eol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo_zone_pv = pd.read_feather(os.path.join(data_folder, \"meteo_zone_echeance12_2016_2020_HRES_piPV_smooth.feather\"))\n",
    "print(f\"echeances:{sorted(df_meteo_zone_pv.echeance.unique())}\") # echeance 0min - 11h30\n",
    "print(f\"zones:{sorted(df_meteo_zone_pv.zone.unique())}\") # echeance 0min - 11h30\n",
    "assert df_meteo_zone_pv.isnull().sum().sum() == 0 # No missing value\n",
    "df_meteo_zone_pv.rename(columns={\"date_lancement_meteo\": \"date_lancement\"}, inplace=True)\n",
    "fix_echeance(df_meteo_zone_pv)\n",
    "\n",
    "# Long to large\n",
    "other_cols = [\"tcc\",\"t2m\",\"ssrd\",\"ff100\",\"u100\",\"v100\"]\n",
    "df_meteo_zone_pv = df_meteo_zone_pv.pivot(index=[\"date_lancement\",\"date_cible\",\"echeance\"], values=other_cols, columns=\"zone\").reset_index()\n",
    "assert df_meteo_zone_pv.isnull().sum().sum() == 0 # No missing value\n",
    "df_meteo_zone_pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prodpv_fc_q90 = pd.read_feather(os.path.join(data_folder, \"productionPV_FC_cielclair_q90.feather\"))\n",
    "df_prodpv_fc_q90.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df_prev_sans_obs2020\n",
    "\n",
    "# DROP ECHEANCES > 4\n",
    "df = df[df.echeance <= 4.0]\n",
    "\n",
    "df_conso = df[df.type =='consommation'].drop(columns='type')\n",
    "df_pv = df[df.type =='photovoltaique'].drop(columns='type')\n",
    "df_conso_res = df[df.type =='consommation_residuelle'].drop(columns='type')\n",
    "df_eol = df[df.type =='eolien'].drop(columns='type')\n",
    "# No missing data in year < 2020, prev\n",
    "assert percent_rows_na(df_eol[df_eol.date_cible.dt.year<2020])==0.0 # No missing value in train\n",
    "assert percent_rows_na(df_pv[df_pv.date_cible.dt.year<2020])==0.0 # No missing value in train\n",
    "assert percent_rows_na(df_conso_res[df_conso_res.date_cible.dt.year<2020])==0.0 # No missing value in train\n",
    "assert percent_rows_na(df_conso[df_conso.date_cible.dt.year<2020])==0.0 # No missing value in train\n",
    "\n",
    "df_pv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PV\n",
    "PV_USELESS_COLS = ['ff100','u100','v100']\n",
    "\n",
    "df_pv_meteo = df_pv.merge(df_meteo_zone_pv.drop(columns=PV_USELESS_COLS), on=['date_cible','date_lancement'], how='inner')\n",
    "print(f\"\"\"\n",
    "      {percent_rows_na(df_pv_meteo)} % rows with missing values.\n",
    "      They come from merging meteo and prod/conso time series\n",
    "      \"\"\") \n",
    "df_pv_meteo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LONG\n",
    "EOL_USELESS_COLS = ['tcc','ssrd','t2m']\n",
    "df_eol_meteo = df_eol.merge(df_meteo_zone_eol.drop(columns=EOL_USELESS_COLS), on=['date_cible','date_lancement'], how='inner')\n",
    "# TODO check how many values are lost during inner join\n",
    "df_eol_meteo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSO_USELESS_COLS = ['ff100','u100','v100']\n",
    "# WARNING: TODO USE REAL WEATHER DATA \n",
    "\n",
    "\n",
    "df_conso_meteo = df_conso.merge(df_meteo_zone_pv.drop(columns=PV_USELESS_COLS), on=['date_cible','date_lancement'], how='inner')\n",
    "df_conso_meteo\n",
    "\n",
    "\n",
    "df_conso_res_meteo = df_conso_res.merge(df_meteo_zone_pv.drop(columns=PV_USELESS_COLS), on=['date_cible','date_lancement'], how='inner')\n",
    "df_conso_res_meteo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv_meteo.to_hdf(\"./features/photovoltaique.hdf\",key=\"data\")\n",
    "df_eol_meteo.to_hdf(\"./features/eolien.hdf\",key=\"data\")\n",
    "df_conso_meteo.to_hdf(\"./features/consommation.hdf\",key=\"data\")\n",
    "df_conso_res_meteo.to_hdf(\"./features/consommation_residuelle.hdf\",key=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cgi import test\n",
    "from pathlib import Path\n",
    "import time\n",
    "from unittest import result\n",
    "from pytorch_lightning import Trainer\n",
    "from ray import tune\n",
    "import torch\n",
    "\n",
    "from train import DataModule, Regressor\n",
    "\n",
    "OBS_TYPES = ['photovoltaique','eolien','consommation','consommation_residuelle']\n",
    "\n",
    "def prepare_submission(obs_type, results):\n",
    "    \n",
    "    net=Regressor(results.best_config)\n",
    "    \n",
    "    # Predict quantiles\n",
    "    with results.best_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "        ckp = torch.load(Path(loaded_checkpoint_dir) / \"checkpoint\")\n",
    "        net.load_state_dict(ckp['state_dict'])\n",
    "    \n",
    "    df = pd.read_hdf(f'./features/{obs_type}.hdf')\n",
    "    dm = DataModule(df, label='fc', batch_size=results.best_config['batch_size'])\n",
    "    dm.prepare_data()\n",
    "    net.eval()\n",
    "    outs = net(dm.x_test).detach()\n",
    "    \n",
    "    quantiles_cols = [f\"{level:.3f}\" for level in  np.array(Regressor(results.best_config).quantile_levels)]\n",
    "    quantiles_df = pd.DataFrame(columns=quantiles_cols, data=outs)\n",
    "    \n",
    "    # Concat to original DF\n",
    "    results_df = pd.concat([dm.df_test, quantiles_df.set_index(dm.df_test.index)],axis=1)\n",
    "    \n",
    "    # Remove useless echeances\n",
    "    results_df = results_df[results_df.echeance.isin([0.5,1,2,4])]\n",
    "    \n",
    "    # Large to long\n",
    "    results_df['id'] = results_df.index\n",
    "    results_df = results_df[['date_cible','date_lancement','pi']+quantiles_cols]\n",
    "    results_df =  pd.melt(results_df, id_vars=['date_cible','date_lancement','pi'],value_vars=quantiles_cols,var_name=\"quantile_niveau\",value_name=\"prev_q\")\n",
    "    results_df['quantile_niveau'] = pd.to_numeric(results_df['quantile_niveau'])\n",
    "    results_df['type'] = obs_type\n",
    "    \n",
    "    # Multiply by installed power  / WARNING: for conso, does nothing.\n",
    "    results_df['prev_q'] = results_df['prev_q'] * results_df['pi']\n",
    "    results_df.drop(columns='pi', inplace=True)\n",
    "    \n",
    "    # Zeroing negative productions\n",
    "    results_df.loc[results_df.prev_q < 0, 'prev_q'] = 0 \n",
    "    return results_df\n",
    "        \n",
    "\n",
    "outs = []\n",
    "for obs_type in OBS_TYPES:\n",
    "    exp_path = max(Path(f\"./ray/{obs_type}/\").glob('*experiment*'), key=os.path.getctime) # Hopefully take latest\n",
    "    print(f\"\"\"\n",
    "          Preparing submission for {obs_type}...\n",
    "          Using Experiment {exp_path}\"\"\")\n",
    "    results = tune.ExperimentAnalysis(experiment_checkpoint_path=exp_path,default_metric=\"val/loss\",default_mode=\"min\")\n",
    "    print(results.best_config)\n",
    "\n",
    "    outs.append(prepare_submission(obs_type, results))\n",
    "submission = pd.concat(outs,axis=0)\n",
    "submission.reset_index(inplace=True)\n",
    "submission.drop(columns='index').to_feather(f'./submissions/AR_{time.time()}.feather')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"plotly\"\n",
    "import plotly.express as px\n",
    "\n",
    "px.line(submission[submission.type=='consommation_residuelle'], x='date_cible',y='prev_q',color='quantile_niveau')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"plotly\"\n",
    "import plotly.express as px\n",
    "df_pv_plot = df_prev_sans_obs2020[(df_prev_sans_obs2020.date_cible.dt.year ==2019) & (df_prev_sans_obs2020.type==\"photovoltaique\")]\n",
    "df_pv_plot[df_pv_plot.echeance==0.0]\n",
    "px.line(df_pv_plot, x='date_cible',y='obs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDEAs\n",
    "- facteur de charge \n",
    "- coeff zone meteo ? altitude ? longitude ?\n",
    "- is it damageable to scale features for quantile prediction ?\n",
    "\n",
    "## TODO\n",
    "- add dvc \n",
    "- compute score for best models on val set \n",
    "- add prev to visualisation of outputs\n",
    "\n",
    "### features\n",
    "\n",
    "- ssrd\n",
    "- tcc\t\n",
    "t2m\t\n",
    "ssrd\t\n",
    "ff100\n",
    "u100\n",
    "v100\n",
    "echeance\n",
    "prod_installée \n",
    "puissance_installee\n",
    "zone_climatique\n",
    "zone\n",
    "clear_sky_FC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('prev_margins')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ca155f627546645520d33fc1c0c7b9e0b61bcb631c4abd47be2f89dc4da8d2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
