{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import warnings\n",
    "from cgi import test\n",
    "from pathlib import Path\n",
    "import time\n",
    "from unittest import result\n",
    "from pytorch_lightning import Trainer\n",
    "from ray import tune\n",
    "import torch\n",
    "\n",
    "from train import DataModule, Regressor\n",
    "\n",
    "sn.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from functools import partial\n",
    "from vacances_scolaires_france import SchoolHolidayDates\n",
    "calendar = SchoolHolidayDates()\n",
    "\n",
    "ALLOWED_HORIZONS = [0.5,1,2,4]\n",
    "\n",
    "other_cols = [\"tcc\",\"t2m\",\"ssrd\",\"ff100\",\"u100\",\"v100\"]\n",
    "data_folder = \"./data_challenge/data\"\n",
    "def percent_rows_na(df):\n",
    "    return (len(df)-len(df.dropna(axis=0)))*100/len(df)\n",
    "\n",
    "\n",
    "def assign_meteo_date_lancement(dt):    \n",
    "    \n",
    "    if 0<= dt.hour < 6:\n",
    "         hour_date_lancement = 0\n",
    "    elif 6< dt.hour <= 12:\n",
    "         hour_date_lancement = 6\n",
    "    elif 12< dt.hour <= 18:\n",
    "         hour_date_lancement = 12\n",
    "    else:\n",
    "         hour_date_lancement = 18\n",
    "    return dt.replace(hour=hour_date_lancement)\n",
    "         \n",
    "         \n",
    "def add_holiday_and_weekends_indicators(df):\n",
    "    # WARNING : use only for conso !\n",
    "    df['is_weekend'] = (df.date_cible.dt.dayofweek > 4).astype(int).astype(float)\n",
    "    df['date'] = df.date_cible.dt.date\n",
    "    my_holiday_func = lambda zone, date : calendar.is_holiday_for_zone(date, zone)\n",
    "    df['is_holiday_A'] = df.date.apply(partial(my_holiday_func,'A')).astype(int).astype(float)\n",
    "    df['is_holiday_B'] = df.date.apply(partial(my_holiday_func,'B')).astype(int).astype(float)\n",
    "    df['is_holiday_C'] = df.date.apply(partial(my_holiday_func,'C')).astype(int).astype(float)\n",
    "    df.drop(columns='date',inplace=True)\n",
    "    return df\n",
    "    \n",
    "def fix_echeance(df):\n",
    "    df['echeance'] = (df.date_cible - df.date_lancement).dt.seconds/3600\n",
    "\n",
    "def remove_useless_horizons(df):\n",
    "    return df.loc[df.echeance.isin(ALLOWED_HORIZONS)]\n",
    "\n",
    "def add_datetime_features(df):\n",
    "    # time in the year\n",
    "    #df['year_dt'] =  datetime.datetime(year=df.date_cible.dt.year)\n",
    "    tzinfo = df.date_cible.dt.tz\n",
    "    df['tiy'] = (df.date_cible - df.date_cible.dt.year.apply(lambda y: datetime.datetime(year=y,month=1,day=1,tzinfo=tzinfo))).dt.total_seconds()/(365*24*60*60)\n",
    "    # time in the day\n",
    "    df['tid'] = (df.date_cible.dt.hour *3600 + df.date_cible.dt.minute *60 + df.date_cible.dt.second)/(24*60*60)\n",
    "    # TODO: type of day for consumption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_station = pd.read_csv(os.path.join(data_folder,\"liste_stations.csv\"), sep=\";\", header=0)\n",
    "df_list_station.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prev_sans_obs2020 = pd.read_feather(os.path.join(data_folder, \"df_prev_sans_obs2020.feather\"))\n",
    "print(df_prev_sans_obs2020.echeance.unique()) # echeance 30min - 7h\n",
    "print(df_prev_sans_obs2020.isnull().sum()) # Missing 417844 observations (for 2020)\n",
    "# Add fake PI for conso in order to get a FC between 0 and 1\n",
    "df_prev_sans_obs2020.loc[df_prev_sans_obs2020.type.str.contains('conso'),'pi'] = df_prev_sans_obs2020[df_prev_sans_obs2020.type.str.contains('conso')].obs.max() + 10**5\n",
    "# Compute FCs\n",
    "df_prev_sans_obs2020['obs_fc'] = df_prev_sans_obs2020['obs'] / df_prev_sans_obs2020['pi']\n",
    "df_prev_sans_obs2020['prev_fc'] = df_prev_sans_obs2020['prev'] / df_prev_sans_obs2020['pi']\n",
    "df_prev_sans_obs2020['error_fc'] = df_prev_sans_obs2020['obs_fc'] - df_prev_sans_obs2020['prev_fc']\n",
    "\n",
    "add_datetime_features(df_prev_sans_obs2020)\n",
    "df_prev_sans_obs2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grille_zoneclimat_fin18 = pd.read_feather(os.path.join(data_folder, \"grille_zone_climatique_fin2018.feather\"))\n",
    "df_grille_zoneclimat_fin18.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo_zone_eol = pd.read_feather(os.path.join(data_folder, \"meteo_zone_echeance12_2016_2020_HRES_piEOL_smooth.feather\"))\n",
    "df_meteo_zone_eol.groupby('date_lancement_meteo').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo_zone_eol = pd.read_feather(os.path.join(data_folder, \"meteo_zone_echeance12_2016_2020_HRES_piEOL_smooth.feather\"))\n",
    "print(sorted(df_meteo_zone_eol.echeance.unique())) # echeance 0min - 11h30\n",
    "assert df_meteo_zone_eol.isnull().sum().sum() == 0 # No missing value\n",
    "# Long to large\n",
    "\n",
    "df_meteo_zone_eol = df_meteo_zone_eol.pivot(index=[\"date_lancement_meteo\",\"date_cible\",\"echeance\"], values=other_cols, columns=\"zone\").reset_index()\n",
    "assert df_meteo_zone_eol.isnull().sum().sum() == 0 # No missing value\n",
    "df_meteo_zone_eol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo_zone_pv = pd.read_feather(os.path.join(data_folder, \"meteo_zone_echeance12_2016_2020_HRES_piPV_smooth.feather\"))\n",
    "print(f\"echeances:{sorted(df_meteo_zone_pv.echeance.unique())}\") # echeance 0min - 11h30\n",
    "print(f\"zones:{sorted(df_meteo_zone_pv.zone.unique())}\") # echeance 0min - 11h30\n",
    "assert df_meteo_zone_pv.isnull().sum().sum() == 0 # No missing value\n",
    "\n",
    "\n",
    "# Long to large\n",
    "other_cols = [\"tcc\",\"t2m\",\"ssrd\",\"ff100\",\"u100\",\"v100\"]\n",
    "df_meteo_zone_pv = df_meteo_zone_pv.pivot(index=[\"date_lancement_meteo\",\"date_cible\",\"echeance\"], values=other_cols, columns=\"zone\").reset_index()\n",
    "assert df_meteo_zone_pv.isnull().sum().sum() == 0 # No missing value\n",
    "df_meteo_zone_pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prodpv_fc_q90 = pd.read_feather(os.path.join(data_folder, \"productionPV_FC_cielclair_q90.feather\"))\n",
    "df_prodpv_fc_q90.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df = remove_useless_horizons(df_prev_sans_obs2020)\n",
    "df = df_prev_sans_obs2020\n",
    "\n",
    "# Drop uselss horizons\n",
    "df = df[df.echeance.isin([0.5,1,2,4])]\n",
    "\n",
    "df['date_lancement_meteo'] = df.date_lancement.apply(assign_meteo_date_lancement)\n",
    "df_pv = df[df.type =='photovoltaique'].drop(columns='type')\n",
    "df_conso = df[df.type =='consommation'].drop(columns='type')\n",
    "df_conso_res = df[df.type =='consommation_residuelle'].drop(columns='type')\n",
    "df_eol = df[df.type =='eolien'].drop(columns='type')\n",
    "# No missing data in year < 2020, prev\n",
    "assert percent_rows_na(df_eol[df_eol.date_cible.dt.year<2020])==0.0 # No missing value in train\n",
    "assert percent_rows_na(df_pv[df_pv.date_cible.dt.year<2020])==0.0 # No missing value in train\n",
    "assert percent_rows_na(df_conso_res[df_conso_res.date_cible.dt.year<2020])==0.0 # No missing value in train\n",
    "assert percent_rows_na(df_conso[df_conso.date_cible.dt.year<2020])==0.0 # No missing value in train\n",
    "\n",
    "# Add holidays for conso\n",
    "df_conso = add_holiday_and_weekends_indicators(df_conso) # SLOW\n",
    "df_conso_res = add_holiday_and_weekends_indicators(df_conso_res) # SLOW\n",
    "\n",
    "\n",
    "\n",
    "df_pv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PV\n",
    "PV_USELESS_COLS = ['ff100','u100','v100']\n",
    "df_pv_meteo = df_pv.merge(df_meteo_zone_pv.drop(columns=PV_USELESS_COLS+['echeance']), on=['date_cible','date_lancement_meteo'], how='inner').drop(columns='date_lancement_meteo')\n",
    "\n",
    "df_pv_meteo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LONG\n",
    "EOL_USELESS_COLS = ['tcc','ssrd','t2m']\n",
    "df_eol_meteo = df_eol.merge(df_meteo_zone_eol.drop(columns=EOL_USELESS_COLS+['echeance']), on=['date_cible','date_lancement_meteo'], how='inner').drop(columns='date_lancement_meteo')\n",
    "# TODO check how many values are lost during inner join\n",
    "df_eol_meteo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSO_USELESS_COLS = ['ff100','u100','v100']\n",
    "# WARNING: TODO USE REAL WEATHER DATA \n",
    "\n",
    "\n",
    "df_conso_meteo = df_conso.merge(df_meteo_zone_pv.drop(columns=PV_USELESS_COLS+['echeance']), on=['date_cible','date_lancement_meteo'], how='inner').drop(columns='date_lancement_meteo')\n",
    "df_conso_meteo\n",
    "\n",
    "\n",
    "df_conso_res_meteo = df_conso_res.merge(df_meteo_zone_pv.drop(columns=PV_USELESS_COLS+['echeance']), on=['date_cible','date_lancement_meteo'], how='inner').drop(columns='date_lancement_meteo')\n",
    "df_conso_res_meteo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv_meteo.to_hdf(\"./features/photovoltaique.hdf\",key=\"data\")\n",
    "df_eol_meteo.to_hdf(\"./features/eolien.hdf\",key=\"data\")\n",
    "df_conso_meteo.to_hdf(\"./features/consommation.hdf\",key=\"data\")\n",
    "df_conso_res_meteo.to_hdf(\"./features/consommation_residuelle.hdf\",key=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proving that all input data have hourly resolution\n",
    "px.line(df_conso_res_meteo[df_conso_res_meteo.echeance==1.0].sort_values('date_cible'),x='date_cible',y=\"prev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from train import LABEL_NAME\n",
    "\n",
    "\n",
    "OBS_TYPES = ['photovoltaique','eolien','consommation','consommation_residuelle']\n",
    "SUBMISSION_COLS = ['date_cible','date_lancement','quantile_niveau','type','prev_q']\n",
    "def prepare_submission(obs_type, results, df_type='test'):\n",
    "    # TODO fix error here\n",
    "    net=Regressor(results.best_config)\n",
    "\n",
    "    # Predict quantiles - using dm.predict_loader would be cleaner but does not work....\n",
    "    with results.best_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "        ckp = torch.load(Path(loaded_checkpoint_dir) / \"checkpoint\")\n",
    "        net.load_state_dict(ckp['state_dict'])\n",
    "    \n",
    "    # Load data\n",
    "    OG_df = pd.read_hdf(f'./features/{obs_type}.hdf')\n",
    "    dm = DataModule(OG_df, label=LABEL_NAME, batch_size=results.best_config['batch_size'])\n",
    "    dm.prepare_data()\n",
    "    x = getattr(dm, \"x_\"+df_type)\n",
    "    df = getattr(dm, \"df_\"+df_type)\n",
    "    \n",
    "    # Predict\n",
    "    net.eval()\n",
    "    outs = net(x).detach()\n",
    "    \n",
    "    quantiles_cols = [f\"{level:.3f}\" for level in  np.array(net.quantile_levels)]\n",
    "    quantiles_df = pd.DataFrame(columns=quantiles_cols, data=outs)\n",
    "    \n",
    "    # Concat to original DF\n",
    "    results_df = pd.concat([df, quantiles_df.set_index(df.index)],axis=1)\n",
    "    \n",
    "    # Remove useless echeances\n",
    "    results_df = results_df[results_df.echeance.isin(ALLOWED_HORIZONS)]\n",
    "    \n",
    "    # Large to long\n",
    "    results_df['id'] = results_df.index\n",
    "    COLS_TO_KEEP = ['date_cible','date_lancement','pi','echeance','prev','obs']\n",
    "    for col in quantiles_cols:\n",
    "        results_df[col] += results_df['prev_fc']\n",
    "    results_df = results_df[COLS_TO_KEEP+quantiles_cols]\n",
    "    results_df =  pd.melt(results_df, id_vars=COLS_TO_KEEP,value_vars=quantiles_cols,var_name=\"quantile_niveau\",value_name=\"prev_q\")\n",
    "    results_df['quantile_niveau'] = pd.to_numeric(results_df['quantile_niveau'])\n",
    "    results_df['type'] = obs_type\n",
    "    \n",
    "    # Multiply by installed power \n",
    "    results_df['prev_q'] = results_df['prev_q'] * results_df['pi']\n",
    "    results_df.drop(columns='pi', inplace=True)\n",
    "    # Zeroing negative productions\n",
    "    results_df.loc[results_df.prev_q < 0, 'prev_q'] = 0 \n",
    "    return results_df\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_EXP_BY_TYPE = dict(\n",
    "    photovoltaique=\"./ray/photovoltaique/experiment_state-2022-09-07_17-34-41.json\",\n",
    "    eolien=\"./ray/eolien/experiment_state-2022-09-07_16-47-39.json\",\n",
    "    consommation=\"./ray/consommation/experiment_state-2022-09-07_17-05-00.json\",\n",
    "    consommation_residuelle=\"./ray/consommation_residuelle/experiment_state-2022-09-07_17-28-04.json\"\n",
    ")\n",
    "\n",
    "outs = []\n",
    "for obs_type in OBS_TYPES:\n",
    "    # Best model so far\n",
    "    run_to_beat = tune.ExperimentAnalysis(experiment_checkpoint_path=BEST_EXP_BY_TYPE[obs_type],default_metric=\"val/loss\",default_mode=\"min\")\n",
    "    \n",
    "    # Latest run \n",
    "    exp_path = max(Path(f\"./ray/{obs_type}/\").glob('*experiment*'), key=os.path.getctime) # Hopefully take latest\n",
    "    results = tune.ExperimentAnalysis(experiment_checkpoint_path=exp_path,default_metric=\"val/loss\",default_mode=\"min\")\n",
    "    print(f\"\"\"\n",
    "          Preparing submission for {obs_type}...\n",
    "          Using Experiment {exp_path}\n",
    "          Best_loss so far: {run_to_beat.best_result['val/loss']}\n",
    "          Validation loss: {results.best_result['val/loss']}\n",
    "          Relative val loss increase (%; negative is good): {(results.best_result['val/loss'] - run_to_beat.best_result['val/loss'])*100/run_to_beat.best_result['val/loss']}\n",
    "          \"\"\")\n",
    "    \n",
    "\n",
    "    outs.append(prepare_submission(obs_type, results))\n",
    "submission = pd.concat(outs,axis=0)\n",
    "submission.reset_index(inplace=True)\n",
    "submission[SUBMISSION_COLS].to_feather(f'./submissions/AR_{time.time()}.feather', compression=\"zstd\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"plotly\"\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def plot_val_output(obs_type):\n",
    "        # Plot val preds. NB: there are holes because of the split between train and val.\n",
    "    exp_path = max(Path(f\"./ray/{obs_type}/\").glob('*experiment*'), key=os.path.getctime) # take latest\n",
    "    results = tune.ExperimentAnalysis(experiment_checkpoint_path=exp_path,default_metric=\"val/loss\",default_mode=\"min\")\n",
    "    print(f\"\"\"\n",
    "            Preparing submission for {obs_type}...\n",
    "            Using Experiment {exp_path}\n",
    "            \n",
    "            \"\"\")\n",
    "    print(results.best_config)\n",
    "\n",
    "    out = prepare_submission(obs_type, results, df_type='val')\n",
    "    df_conso_sub = out[(out.quantile_niveau.isin([0.25, 0.75]))& (out.echeance==1.0)]\n",
    "    df_conso_sub.sort_values(by='date_cible',inplace=True)\n",
    "\n",
    "    fig = px.line(df_conso_sub, x='date_cible',y='prev_q',color='quantile_niveau', line_dash_sequence=['dash'])\n",
    "    fig.add_trace(go.Scatter(x=df_conso_sub.date_cible,y=df_conso_sub.prev,name=\"prev\",line=dict(width=4)))\n",
    "    fig.add_trace(go.Scatter(x=df_conso_sub.date_cible,y=df_conso_sub.obs,name=\"obs\",line=dict(width=4)))\n",
    "    fig.update_traces(connectgaps=False) \n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_output('consommation_residuelle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_output('consommation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_output('eolien')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.df_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_fiability(out):\n",
    "    quantile_levels = np.arange(0.005,1.00,0.005)\n",
    "    out['percentage'] = pd.NA\n",
    "    # TODO: 1 select quantile\n",
    "    # 2 compute % of distribution above it\n",
    "    # 3 deduce loss\n",
    "    # LOSS \n",
    "    out['percentage'] = out.apply(lambda row: (row.prev_q >= out.obs).sum()/len(out), axis=1)\n",
    "    out['reliability'] = indicator(out['percentage'] - out['prev_q'])*10\n",
    "    \n",
    "    return out\n",
    "analyze_fiability(out.head(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug submission\n",
    "obs_type = 'consommation'\n",
    "df_type='train'\n",
    "exp_path = max(Path(f\"./ray/{obs_type}/\").glob('*experiment*'), key=os.path.getctime) # take latest\n",
    "results = tune.ExperimentAnalysis(experiment_checkpoint_path=exp_path,default_metric=\"val/loss\",default_mode=\"min\")\n",
    "net=Regressor(results.best_config)\n",
    "\n",
    "# Predict quantiles - using dm.predict_loader would be cleaner but does not work....\n",
    "with results.best_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "    ckp = torch.load(Path(loaded_checkpoint_dir) / \"checkpoint\")\n",
    "    net.load_state_dict(ckp['state_dict'])\n",
    "\n",
    "# Load data\n",
    "OG_df = pd.read_hdf(f'./features/{obs_type}.hdf')\n",
    "dm = DataModule(OG_df, label='error_fc', batch_size=results.best_config['batch_size'])\n",
    "dm.prepare_data()\n",
    "x = getattr(dm, \"x_\"+df_type)\n",
    "df = getattr(dm, \"df_\"+df_type)\n",
    "\n",
    "px.line(df[df.echeance==1.0].sort_values('date_cible'),x='date_cible',y='prev')\n",
    "# # Predict\n",
    "# net.eval()\n",
    "# outs = net(x).detach()\n",
    "\n",
    "# print(f\"\"\"\n",
    "#         Validation loss (centered): {results.best_result['val/loss']}\n",
    "#         \"\"\")\n",
    "\n",
    "\n",
    "# quantiles_cols = [f\"{level:.3f}\" for level in  np.array(net.quantile_levels)]\n",
    "# quantiles_df = pd.DataFrame(columns=quantiles_cols, data=outs)\n",
    "\n",
    "# # Concat to original DF\n",
    "# results_df = pd.concat([df, quantiles_df.set_index(df.index)],axis=1)\n",
    "\n",
    "# # Remove useless echeances\n",
    "# results_df = results_df[results_df.echeance.isin(ALLOWED_HORIZONS)]\n",
    "\n",
    "# # Large to long\n",
    "# results_df['id'] = results_df.index\n",
    "# COLS_TO_KEEP = ['date_cible','date_lancement','pi','echeance','prev','obs']\n",
    "# for col in quantiles_cols:\n",
    "#     results_df[col] += results_df['prev_fc']\n",
    "# results_df = results_df[COLS_TO_KEEP+quantiles_cols]\n",
    "# results_df =  pd.melt(results_df, id_vars=COLS_TO_KEEP,value_vars=quantiles_cols,var_name=\"quantile_niveau\",value_name=\"prev_q\")\n",
    "# results_df['quantile_niveau'] = pd.to_numeric(results_df['quantile_niveau'])\n",
    "# results_df['type'] = obs_type\n",
    "\n",
    "# # Multiply by installed power \n",
    "# results_df['prev_q'] = results_df['prev_q'] * results_df['pi']\n",
    "# results_df.drop(columns='pi', inplace=True)\n",
    "# # Zeroing negative productions\n",
    "# results_df.loc[results_df.prev_q < 0, 'prev_q'] = 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('features/consommation.hdf')\n",
    "dm = DataModule(df, LABEL_NAME,10)\n",
    "dm.prepare_data()\n",
    "# Show feature columns\n",
    "print(dm.df_train.drop(columns=dm.cols_to_drop))\n",
    "dm.df_train[LABEL_NAME].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that df has all timesteps\n",
    "dm.df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDEAs\n",
    "- coeff zone meteo ? altitude ? longitude ?\n",
    "- is it damageable to scale features for quantile prediction ?\n",
    "- train only on useful horizons ?\n",
    "- add dvc \n",
    "\n",
    "## TODO\n",
    "- Compute \"real\" score on best models (* PI) \n",
    "- add fiabilitÃ©\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('prev_margins')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ca155f627546645520d33fc1c0c7b9e0b61bcb631c4abd47be2f89dc4da8d2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
