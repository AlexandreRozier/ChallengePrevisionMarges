{
  "checkpoints": [
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00002\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 2,\n    \"lr\": 0.07362564490432842,\n    \"dropout_rate\": 0.2289684682829076,\n    \"batch_size\": 32,\n    \"epochs\": 6\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 2,\n    \"lr\": 0.07362564490432842,\n    \"dropout_rate\": 0.2289684682829076,\n    \"batch_size\": 32,\n    \"epochs\": 6\n  },\n  \"experiment_tag\": \"2_batch_size=32,dropout_rate=0.2290,epochs=6,layer_1=2,lr=0.0736\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"trial_id\": \"7ad3d_00002\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626262.6142573,\n  \"relative_logdir\": \"train_with_config_7ad3d_00002_2_batch_size=32,dropout_rate=0.2290,epochs=6,layer_1=2,lr=0.0736_2022-09-08_10-37-42\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00002_2_batch_size=32,dropout_rate=0.2290,epochs=6,layer_1=2,lr=0.0736_2022-09-08_10-37-42/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00002_2_batch_size=32,dropout_rate=0.2290,epochs=6,layer_1=2,lr=0.0736_2022-09-08_10-37-42/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00003\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 2,\n    \"lr\": 0.002949033326341135,\n    \"dropout_rate\": 0.09254263019018688,\n    \"batch_size\": 32,\n    \"epochs\": 8\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 2,\n    \"lr\": 0.002949033326341135,\n    \"dropout_rate\": 0.09254263019018688,\n    \"batch_size\": 32,\n    \"epochs\": 8\n  },\n  \"experiment_tag\": \"3_batch_size=32,dropout_rate=0.0925,epochs=8,layer_1=2,lr=0.0029\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"trial_id\": \"7ad3d_00003\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626262.6231186,\n  \"relative_logdir\": \"train_with_config_7ad3d_00003_3_batch_size=32,dropout_rate=0.0925,epochs=8,layer_1=2,lr=0.0029_2022-09-08_10-37-42\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00003_3_batch_size=32,dropout_rate=0.0925,epochs=8,layer_1=2,lr=0.0029_2022-09-08_10-37-42/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00003_3_batch_size=32,dropout_rate=0.0925,epochs=8,layer_1=2,lr=0.0029_2022-09-08_10-37-42/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00005\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 4,\n    \"lr\": 0.013351122734879116,\n    \"dropout_rate\": 0.1387179277506562,\n    \"batch_size\": 64,\n    \"epochs\": 2\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 4,\n    \"lr\": 0.013351122734879116,\n    \"dropout_rate\": 0.1387179277506562,\n    \"batch_size\": 64,\n    \"epochs\": 2\n  },\n  \"experiment_tag\": \"5_batch_size=64,dropout_rate=0.1387,epochs=2,layer_1=4,lr=0.0134\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"trial_id\": \"7ad3d_00005\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626262.6384027,\n  \"relative_logdir\": \"train_with_config_7ad3d_00005_5_batch_size=64,dropout_rate=0.1387,epochs=2,layer_1=4,lr=0.0134_2022-09-08_10-37-42\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00005_5_batch_size=64,dropout_rate=0.1387,epochs=2,layer_1=4,lr=0.0134_2022-09-08_10-37-42/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00005_5_batch_size=64,dropout_rate=0.1387,epochs=2,layer_1=4,lr=0.0134_2022-09-08_10-37-42/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00004\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 4,\n    \"lr\": 0.00024734503415676525,\n    \"dropout_rate\": 0.12111726182657377,\n    \"batch_size\": 128,\n    \"epochs\": 1\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 4,\n    \"lr\": 0.00024734503415676525,\n    \"dropout_rate\": 0.12111726182657377,\n    \"batch_size\": 128,\n    \"epochs\": 1\n  },\n  \"experiment_tag\": \"4_batch_size=128,dropout_rate=0.1211,epochs=1,layer_1=4,lr=0.0002\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"trial_id\": \"7ad3d_00004\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626262.6307213,\n  \"relative_logdir\": \"train_with_config_7ad3d_00004_4_batch_size=128,dropout_rate=0.1211,epochs=1,layer_1=4,lr=0.0002_2022-09-08_10-37-42\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00004_4_batch_size=128,dropout_rate=0.1211,epochs=1,layer_1=4,lr=0.0002_2022-09-08_10-37-42/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00004_4_batch_size=128,dropout_rate=0.1211,epochs=1,layer_1=4,lr=0.0002_2022-09-08_10-37-42/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00000\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 2,\n    \"lr\": 0.0007842375647005945,\n    \"dropout_rate\": 0.0722630907866542,\n    \"batch_size\": 32,\n    \"epochs\": 7\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 2,\n    \"lr\": 0.0007842375647005945,\n    \"dropout_rate\": 0.0722630907866542,\n    \"batch_size\": 32,\n    \"epochs\": 7\n  },\n  \"experiment_tag\": \"0_batch_size=32,dropout_rate=0.0723,epochs=7,layer_1=2,lr=0.0008\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {},\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626251.988336,\n  \"relative_logdir\": \"train_with_config_7ad3d_00000_0_batch_size=32,dropout_rate=0.0723,epochs=7,layer_1=2,lr=0.0008_2022-09-08_10-37-31\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00000_0_batch_size=32,dropout_rate=0.0723,epochs=7,layer_1=2,lr=0.0008_2022-09-08_10-37-31/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00000_0_batch_size=32,dropout_rate=0.0723,epochs=7,layer_1=2,lr=0.0008_2022-09-08_10-37-31/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00001\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 2,\n    \"lr\": 0.009341974836061672,\n    \"dropout_rate\": 0.2785463915616194,\n    \"batch_size\": 64,\n    \"epochs\": 3\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 2,\n    \"lr\": 0.009341974836061672,\n    \"dropout_rate\": 0.2785463915616194,\n    \"batch_size\": 64,\n    \"epochs\": 3\n  },\n  \"experiment_tag\": \"1_batch_size=64,dropout_rate=0.2785,epochs=3,layer_1=2,lr=0.0093\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {\n    \"trial_id\": \"7ad3d_00001\"\n  },\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626258.4219408,\n  \"relative_logdir\": \"train_with_config_7ad3d_00001_1_batch_size=64,dropout_rate=0.2785,epochs=3,layer_1=2,lr=0.0093_2022-09-08_10-37-38\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00001_1_batch_size=64,dropout_rate=0.2785,epochs=3,layer_1=2,lr=0.0093_2022-09-08_10-37-38/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00001_1_batch_size=64,dropout_rate=0.2785,epochs=3,layer_1=2,lr=0.0093_2022-09-08_10-37-38/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00008\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 8,\n    \"lr\": 0.00011119246274648908,\n    \"dropout_rate\": 0.18705962455610667,\n    \"batch_size\": 32,\n    \"epochs\": 3\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 8,\n    \"lr\": 0.00011119246274648908,\n    \"dropout_rate\": 0.18705962455610667,\n    \"batch_size\": 32,\n    \"epochs\": 3\n  },\n  \"experiment_tag\": \"8_batch_size=32,dropout_rate=0.1871,epochs=3,layer_1=8,lr=0.0001\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {},\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626272.779395,\n  \"relative_logdir\": \"train_with_config_7ad3d_00008_8_batch_size=32,dropout_rate=0.1871,epochs=3,layer_1=8,lr=0.0001_2022-09-08_10-37-52\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00008_8_batch_size=32,dropout_rate=0.1871,epochs=3,layer_1=8,lr=0.0001_2022-09-08_10-37-52/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00008_8_batch_size=32,dropout_rate=0.1871,epochs=3,layer_1=8,lr=0.0001_2022-09-08_10-37-52/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00006\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 4,\n    \"lr\": 0.00013140249960314642,\n    \"dropout_rate\": 0.27855087228163583,\n    \"batch_size\": 128,\n    \"epochs\": 6\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 4,\n    \"lr\": 0.00013140249960314642,\n    \"dropout_rate\": 0.27855087228163583,\n    \"batch_size\": 128,\n    \"epochs\": 6\n  },\n  \"experiment_tag\": \"6_batch_size=128,dropout_rate=0.2786,epochs=6,layer_1=4,lr=0.0001\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {},\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626262.6977758,\n  \"relative_logdir\": \"train_with_config_7ad3d_00006_6_batch_size=128,dropout_rate=0.2786,epochs=6,layer_1=4,lr=0.0001_2022-09-08_10-37-42\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00006_6_batch_size=128,dropout_rate=0.2786,epochs=6,layer_1=4,lr=0.0001_2022-09-08_10-37-42/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00006_6_batch_size=128,dropout_rate=0.2786,epochs=6,layer_1=4,lr=0.0001_2022-09-08_10-37-42/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00007\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 16,\n    \"lr\": 0.00016926670200147215,\n    \"dropout_rate\": 0.23675425459442728,\n    \"batch_size\": 64,\n    \"epochs\": 1\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 16,\n    \"lr\": 0.00016926670200147215,\n    \"dropout_rate\": 0.23675425459442728,\n    \"batch_size\": 64,\n    \"epochs\": 1\n  },\n  \"experiment_tag\": \"7_batch_size=64,dropout_rate=0.2368,epochs=1,layer_1=16,lr=0.0002\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {},\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626267.885708,\n  \"relative_logdir\": \"train_with_config_7ad3d_00007_7_batch_size=64,dropout_rate=0.2368,epochs=1,layer_1=16,lr=0.0002_2022-09-08_10-37-47\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00007_7_batch_size=64,dropout_rate=0.2368,epochs=1,layer_1=16,lr=0.0002_2022-09-08_10-37-47/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00007_7_batch_size=64,dropout_rate=0.2368,epochs=1,layer_1=16,lr=0.0002_2022-09-08_10-37-47/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00010\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 4,\n    \"lr\": 0.008873414039076828,\n    \"dropout_rate\": 0.09407402191175618,\n    \"batch_size\": 32,\n    \"epochs\": 3\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 4,\n    \"lr\": 0.008873414039076828,\n    \"dropout_rate\": 0.09407402191175618,\n    \"batch_size\": 32,\n    \"epochs\": 3\n  },\n  \"experiment_tag\": \"10_batch_size=32,dropout_rate=0.0941,epochs=3,layer_1=4,lr=0.0089\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {},\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626272.8320901,\n  \"relative_logdir\": \"train_with_config_7ad3d_00010_10_batch_size=32,dropout_rate=0.0941,epochs=3,layer_1=4,lr=0.0089_2022-09-08_10-37-52\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00010_10_batch_size=32,dropout_rate=0.0941,epochs=3,layer_1=4,lr=0.0089_2022-09-08_10-37-52/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00010_10_batch_size=32,dropout_rate=0.0941,epochs=3,layer_1=4,lr=0.0089_2022-09-08_10-37-52/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00011\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 8,\n    \"lr\": 0.00043899022143141975,\n    \"dropout_rate\": 0.10594739392955646,\n    \"batch_size\": 128,\n    \"epochs\": 3\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 8,\n    \"lr\": 0.00043899022143141975,\n    \"dropout_rate\": 0.10594739392955646,\n    \"batch_size\": 128,\n    \"epochs\": 3\n  },\n  \"experiment_tag\": \"11_batch_size=128,dropout_rate=0.1059,epochs=3,layer_1=8,lr=0.0004\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {},\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"RUNNING\",\n  \"start_time\": 1662626272.8444257,\n  \"relative_logdir\": \"train_with_config_7ad3d_00011_11_batch_size=128,dropout_rate=0.1059,epochs=3,layer_1=8,lr=0.0004_2022-09-08_10-37-52\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"pickled_error_file\": null,\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00012\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 8,\n    \"lr\": 0.0020277961722754974,\n    \"dropout_rate\": 0.3332460877637298,\n    \"batch_size\": 128,\n    \"epochs\": 3\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 8,\n    \"lr\": 0.0020277961722754974,\n    \"dropout_rate\": 0.3332460877637298,\n    \"batch_size\": 128,\n    \"epochs\": 3\n  },\n  \"experiment_tag\": \"12_batch_size=128,dropout_rate=0.3332,epochs=3,layer_1=8,lr=0.0020\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {},\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626272.8732738,\n  \"relative_logdir\": \"train_with_config_7ad3d_00012_12_batch_size=128,dropout_rate=0.3332,epochs=3,layer_1=8,lr=0.0020_2022-09-08_10-37-52\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00012_12_batch_size=128,dropout_rate=0.3332,epochs=3,layer_1=8,lr=0.0020_2022-09-08_10-37-52/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00012_12_batch_size=128,dropout_rate=0.3332,epochs=3,layer_1=8,lr=0.0020_2022-09-08_10-37-52/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00013\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 2,\n    \"lr\": 0.07616988419323747,\n    \"dropout_rate\": 0.24005679180492956,\n    \"batch_size\": 64,\n    \"epochs\": 1\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 2,\n    \"lr\": 0.07616988419323747,\n    \"dropout_rate\": 0.24005679180492956,\n    \"batch_size\": 64,\n    \"epochs\": 1\n  },\n  \"experiment_tag\": \"13_batch_size=64,dropout_rate=0.2401,epochs=1,layer_1=2,lr=0.0762\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {},\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626274.0432284,\n  \"relative_logdir\": \"train_with_config_7ad3d_00013_13_batch_size=64,dropout_rate=0.2401,epochs=1,layer_1=2,lr=0.0762_2022-09-08_10-37-54\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00013_13_batch_size=64,dropout_rate=0.2401,epochs=1,layer_1=2,lr=0.0762_2022-09-08_10-37-54/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00013_13_batch_size=64,dropout_rate=0.2401,epochs=1,layer_1=2,lr=0.0762_2022-09-08_10-37-54/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00015\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 2,\n    \"lr\": 0.0139059937345605,\n    \"dropout_rate\": 0.13999317152649518,\n    \"batch_size\": 128,\n    \"epochs\": 7\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 2,\n    \"lr\": 0.0139059937345605,\n    \"dropout_rate\": 0.13999317152649518,\n    \"batch_size\": 128,\n    \"epochs\": 7\n  },\n  \"experiment_tag\": \"15_batch_size=128,dropout_rate=0.1400,epochs=7,layer_1=2,lr=0.0139\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {},\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626280.5820713,\n  \"relative_logdir\": \"train_with_config_7ad3d_00015_15_batch_size=128,dropout_rate=0.1400,epochs=7,layer_1=2,lr=0.0139_2022-09-08_10-38-00\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00015_15_batch_size=128,dropout_rate=0.1400,epochs=7,layer_1=2,lr=0.0139_2022-09-08_10-38-00/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00015_15_batch_size=128,dropout_rate=0.1400,epochs=7,layer_1=2,lr=0.0139_2022-09-08_10-38-00/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00009\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 4,\n    \"lr\": 0.09476622366825219,\n    \"dropout_rate\": 0.3869897949968659,\n    \"batch_size\": 32,\n    \"epochs\": 8\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 4,\n    \"lr\": 0.09476622366825219,\n    \"dropout_rate\": 0.3869897949968659,\n    \"batch_size\": 32,\n    \"epochs\": 8\n  },\n  \"experiment_tag\": \"9_batch_size=32,dropout_rate=0.3870,epochs=8,layer_1=4,lr=0.0948\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {},\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626272.7882571,\n  \"relative_logdir\": \"train_with_config_7ad3d_00009_9_batch_size=32,dropout_rate=0.3870,epochs=8,layer_1=4,lr=0.0948_2022-09-08_10-37-52\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00009_9_batch_size=32,dropout_rate=0.3870,epochs=8,layer_1=4,lr=0.0948_2022-09-08_10-37-52/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00009_9_batch_size=32,dropout_rate=0.3870,epochs=8,layer_1=4,lr=0.0948_2022-09-08_10-37-52/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00016\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 2,\n    \"lr\": 0.0016575240969010537,\n    \"dropout_rate\": 0.15975718106072323,\n    \"batch_size\": 64,\n    \"epochs\": 4\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 2,\n    \"lr\": 0.0016575240969010537,\n    \"dropout_rate\": 0.15975718106072323,\n    \"batch_size\": 64,\n    \"epochs\": 4\n  },\n  \"experiment_tag\": \"16_batch_size=64,dropout_rate=0.1598,epochs=4,layer_1=2,lr=0.0017\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {},\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626280.6311963,\n  \"relative_logdir\": \"train_with_config_7ad3d_00016_16_batch_size=64,dropout_rate=0.1598,epochs=4,layer_1=2,lr=0.0017_2022-09-08_10-38-00\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00016_16_batch_size=64,dropout_rate=0.1598,epochs=4,layer_1=2,lr=0.0017_2022-09-08_10-38-00/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00016_16_batch_size=64,dropout_rate=0.1598,epochs=4,layer_1=2,lr=0.0017_2022-09-08_10-38-00/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00018\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 2,\n    \"lr\": 0.00303313480254743,\n    \"dropout_rate\": 0.02944284353908091,\n    \"batch_size\": 64,\n    \"epochs\": 8\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 2,\n    \"lr\": 0.00303313480254743,\n    \"dropout_rate\": 0.02944284353908091,\n    \"batch_size\": 64,\n    \"epochs\": 8\n  },\n  \"experiment_tag\": \"18_batch_size=64,dropout_rate=0.0294,epochs=8,layer_1=2,lr=0.0030\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {},\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626281.8463032,\n  \"relative_logdir\": \"train_with_config_7ad3d_00018_18_batch_size=64,dropout_rate=0.0294,epochs=8,layer_1=2,lr=0.0030_2022-09-08_10-38-01\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00018_18_batch_size=64,dropout_rate=0.0294,epochs=8,layer_1=2,lr=0.0030_2022-09-08_10-38-01/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00018_18_batch_size=64,dropout_rate=0.0294,epochs=8,layer_1=2,lr=0.0030_2022-09-08_10-38-01/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00017\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 16,\n    \"lr\": 0.00125961558632154,\n    \"dropout_rate\": 0.3023036915765378,\n    \"batch_size\": 64,\n    \"epochs\": 1\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 16,\n    \"lr\": 0.00125961558632154,\n    \"dropout_rate\": 0.3023036915765378,\n    \"batch_size\": 64,\n    \"epochs\": 1\n  },\n  \"experiment_tag\": \"17_batch_size=64,dropout_rate=0.3023,epochs=1,layer_1=16,lr=0.0013\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {},\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626280.6599145,\n  \"relative_logdir\": \"train_with_config_7ad3d_00017_17_batch_size=64,dropout_rate=0.3023,epochs=1,layer_1=16,lr=0.0013_2022-09-08_10-38-00\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00017_17_batch_size=64,dropout_rate=0.3023,epochs=1,layer_1=16,lr=0.0013_2022-09-08_10-38-00/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00017_17_batch_size=64,dropout_rate=0.3023,epochs=1,layer_1=16,lr=0.0013_2022-09-08_10-38-00/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00014\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 8,\n    \"lr\": 0.009685035842706927,\n    \"dropout_rate\": 0.3484568180953389,\n    \"batch_size\": 128,\n    \"epochs\": 3\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 8,\n    \"lr\": 0.009685035842706927,\n    \"dropout_rate\": 0.3484568180953389,\n    \"batch_size\": 128,\n    \"epochs\": 3\n  },\n  \"experiment_tag\": \"14_batch_size=128,dropout_rate=0.3485,epochs=3,layer_1=8,lr=0.0097\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {},\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626280.5636942,\n  \"relative_logdir\": \"train_with_config_7ad3d_00014_14_batch_size=128,dropout_rate=0.3485,epochs=3,layer_1=8,lr=0.0097_2022-09-08_10-38-00\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00014_14_batch_size=128,dropout_rate=0.3485,epochs=3,layer_1=8,lr=0.0097_2022-09-08_10-38-00/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00014_14_batch_size=128,dropout_rate=0.3485,epochs=3,layer_1=8,lr=0.0097_2022-09-08_10-38-00/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"stub\": false,\n  \"trainable_name\": \"train_with_config\",\n  \"trial_id\": \"7ad3d_00019\",\n  \"config\": {\n    \"input_dim\": 37,\n    \"layer_1\": 4,\n    \"lr\": 0.006416641733972742,\n    \"dropout_rate\": 0.3377991751137111,\n    \"batch_size\": 64,\n    \"epochs\": 7\n  },\n  \"local_dir\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle\",\n  \"evaluated_params\": {\n    \"layer_1\": 4,\n    \"lr\": 0.006416641733972742,\n    \"dropout_rate\": 0.3377991751137111,\n    \"batch_size\": 64,\n    \"epochs\": 7\n  },\n  \"experiment_tag\": \"19_batch_size=64,dropout_rate=0.3378,epochs=7,layer_1=4,lr=0.0064\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059546000000000000008c197261792e74756e652e6578706572696d656e742e747269616c948c095f4c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"placement_group_factory\": \"800595aa000000000000008c237261792e74756e652e657865637574696f6e2e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d948c0343505594473ff000000000000073618c155f686561645f62756e646c655f69735f656d70747994898c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\",\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"_last_result\": {},\n  \"_default_result_or_future\": null,\n  \"last_update_time\": -Infinity,\n  \"metric_analysis\": {},\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {},\n  \"export_formats\": [],\n  \"status\": \"ERROR\",\n  \"start_time\": 1662626292.2675848,\n  \"relative_logdir\": \"train_with_config_7ad3d_00019_19_batch_size=64,dropout_rate=0.3378,epochs=7,layer_1=4,lr=0.0064_2022-09-08_10-38-12\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00019_19_batch_size=64,dropout_rate=0.3378,epochs=7,layer_1=4,lr=0.0064_2022-09-08_10-38-12/error.txt\",\n  \"pickled_error_file\": \"/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/train_with_config_7ad3d_00019_19_batch_size=64,dropout_rate=0.3378,epochs=7,layer_1=4,lr=0.0064_2022-09-08_10-38-12/error.pkl\",\n  \"trial_name_creator\": null,\n  \"trial_dirname_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"custom_syncer\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059587020000000000008c257261792e74756e652e657865637574696f6e2e636865636b706f696e745f6d616e61676572948c125f436865636b706f696e744d616e616765729493942981947d94288c145f636865636b706f696e745f7374726174656779948c0e7261792e6169722e636f6e666967948c10436865636b706f696e74436f6e6669679493942981947d94288c0b6e756d5f746f5f6b656570944e8c1a636865636b706f696e745f73636f72655f617474726962757465948c12747261696e696e675f697465726174696f6e948c16636865636b706f696e745f73636f72655f6f72646572948c036d6178948c14636865636b706f696e745f6672657175656e6379944b008c11636865636b706f696e745f61745f656e64944e75628c155f6c61746573745f636865636b706f696e745f6964944b008c1a5f746f705f7065727369737465645f636865636b706f696e7473945d948c1a5f626573745f7065727369737465645f636865636b706f696e74944e8c1c5f6c61746573745f7065727369737465645f636865636b706f696e74944e8c195f6c61746573745f6d656d6f72795f636865636b706f696e74944e8c185f636865636b706f696e74735f746f5f636c65616e5f7570948f948c195f6e65776573745f6d656d6f72795f636865636b706f696e74948c247261792e6169722e5f696e7465726e616c2e636865636b706f696e745f6d616e61676572948c125f547261636b6564436865636b706f696e749493942981947d94288c0b6469725f6f725f6461746194681b8c11436865636b706f696e7453746f726167659493944b01859452948c026964944e8c0c73746f726167655f6d6f6465944e8c076d657472696373947d948c076e6f64655f6970944e756275622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 1,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}"
  ],
  "runner_data": {
    "_insufficient_resources_manager": {
      "_type": "CLOUDPICKLE_FALLBACK",
      "value": "80059596000000000000008c317261792e74756e652e657865637574696f6e2e696e73756666696369656e745f7265736f75726365735f6d616e61676572948c1d5f496e73756666696369656e745265736f75726365734d616e616765729493942981947d94288c185f6e6f5f72756e6e696e675f747269616c735f73696e6365944affffffff8c0f5f6c6173745f747269616c5f6e756d944affffffff75622e"
    },
    "_max_pending_trials": 16,
    "_metric": null,
    "_total_time": 0,
    "_iteration": 79,
    "_has_errored": true,
    "_fail_fast": false,
    "_server_port": null,
    "_cached_trial_decisions": {},
    "_queued_trial_decisions": {},
    "_should_stop_experiment": false,
    "_local_checkpoint_dir": "/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle",
    "_remote_checkpoint_dir": null,
    "_stopper": {
      "_type": "CLOUDPICKLE_FALLBACK",
      "value": "8005952c000000000000008c157261792e74756e652e73746f707065722e6e6f6f70948c0b4e6f6f7053746f707065729493942981942e"
    },
    "_resumed": false,
    "_start_time": 1662626251.8141396,
    "_last_checkpoint_time": -Infinity,
    "_session_str": "2022-09-08_10-37-31",
    "checkpoint_file": "/home/rozierale/Work/Rte/Challenge_prev_marges/challenge_margin_2022/ray/consommation_residuelle/experiment_state-2022-09-08_10-37-31.json",
    "_checkpoint_period": "auto",
    "launch_web_server": false
  },
  "stats": {
    "start_time": 1662626251.8141396,
    "timestamp": 1662626480.6656942
  }
}